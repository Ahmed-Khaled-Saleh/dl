"""All things related to models (loading, utils,...)"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_model.ipynb.

# %% auto 0
__all__ = ['load_ckpt', 'CustomModel', 'build_model', 'init_model', 'ResNet18WithAttention', 'EfficientNetB0WithAttention']

# %% ../nbs/02_model.ipynb 3
import os

import torch
import torch.nn as nn
from torchvision import transforms, models



# %% ../nbs/02_model.ipynb 4
def load_ckpt(model, ckpt_path):
    if os.path.exists(ckpt_path):
        state_dict = torch.load(ckpt_path, map_location='cpu')
        model.load_state_dict(state_dict)
        print(f"Loaded pretrained weights from {ckpt_path}")
    else:
        print(f"No checkpoint found at {ckpt_path}. Using random initialized weights.")
    return model

# %% ../nbs/02_model.ipynb 5
import torch
import torch.nn as nn
from torchvision import models
class CustomModel(nn.Module):
    def __init__(self, backbone= "resnet18", ckpt= None, num_classes= 3, pretrained=True):
        super().__init__()

        if backbone == "resnet18":
            self.model = models.resnet18(pretrained=pretrained)
            self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)
        elif backbone == "efficientnet":
            self.model = models.efficientnet_b0(pretrained=pretrained)
            self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, num_classes)
        else:
            raise ValueError("Unsupported backbone")
        
        self.model = load_ckpt(self.model, ckpt)
    def forward(self, x):
        if x.ndim == 5:
            x = x.flatten(0, 1)
        return self.model(x)

# %% ../nbs/02_model.ipynb 6
def build_model(backbone="resnet18", ckpt=None, num_classes=3, pretrained=True):
    model = CustomModel(backbone, ckpt, num_classes, pretrained)
    return model

# %% ../nbs/02_model.ipynb 7
def init_model(cfg): 
    
    model = build_model(backbone=cfg.model.backbone, ckpt=cfg.model.ckpt, num_classes=len(cfg.data.label_names), pretrained=cfg.model.pretrained)
    for param in model.parameters():
        param.requires_grad = False

    if cfg.task == "eval":
        return model

    if cfg.task == 'probing':
        if cfg.model.backbone == "resnet18":
            for param in model.model.fc.parameters():
                param.requires_grad = True    
        elif cfg.model.backbone == "efficientnet":
            for param in model.model.classifier[1].parameters():
                param.requires_grad = True

    elif cfg.task == 'fine-tuning':
        for param in model.parameters():
            param.requires_grad = True

    else:
        raise ValueError("Unsupported type. Choose either 'eval', 'probing', or 'fine-tuning'.")

    return model

# %% ../nbs/02_model.ipynb 13
import torch
import torch.nn as nn
from torchvision import models

class ResNet18WithAttention(nn.Module):
    def __init__(self, num_classes=1000, num_heads=8):
        super(ResNet18WithAttention, self).__init__()
        resnet = models.resnet18(weights=None)
        self.backbone = nn.Sequential(*list(resnet.children())[:-2]) # Stops at (B, 512, 8, 8)
        
        self.embed_dim = 512
        
        self.mha = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=num_heads, batch_first=True)
        
        self.ln = nn.LayerNorm(self.embed_dim)
        self.fc = nn.Linear(self.embed_dim, num_classes)

    def forward(self, x):
        # x shape: (Batch, 3, 256, 256)
        features = self.backbone(x) # Shape: (Batch, 512, 8, 8)
        
        # Flatten spatial dimensions: (B, 512, 8, 8) -> (B, 512, 64) -> (B, 64, 512)
        # We treat the 64 pixels as the "sequence"
        b, c, h, w = features.shape
        features = features.view(b, c, h * w).permute(0, 2, 1) 
        
        attn_output, _ = self.mha(features, features, features)
        
        x = self.ln(attn_output + features)
        
        x = x.mean(dim=1)
        
        logits = self.fc(x)
        return logits



# %% ../nbs/02_model.ipynb 17
import torch
import torch.nn as nn
from torchvision import models

class EfficientNetB0WithAttention(nn.Module):
    def __init__(self, num_classes=1000, num_heads=8):
        super(EfficientNetB0WithAttention, self).__init__()
        
        effnet = models.efficientnet_b0(weights=None)
        self.backbone = effnet.features 
        
        self.embed_dim = 1280 
        
        self.mha = nn.MultiheadAttention(
            embed_dim=self.embed_dim, 
            num_heads=num_heads, 
            batch_first=True
        )
        
        self.ln = nn.LayerNorm(self.embed_dim)
        self.fc = nn.Linear(self.embed_dim, num_classes)

    def forward(self, x):
        features = self.backbone(x)
        
        b, c, h, w = features.shape
        features = features.view(b, c, h * w).permute(0, 2, 1)
        
        attn_output, _ = self.mha(features, features, features)
        
        x = self.ln(attn_output + features)
        
        x = x.mean(dim=1) 
        
        logits = self.fc(x)
        return logits


