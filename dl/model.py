"""All things related to models (loading, utils,...)"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_model.ipynb.

# %% auto 0
__all__ = ['build_model', 'load_ckpt', 'init_model', 'ResNet18WithAttention', 'EfficientNetB0WithAttention']

# %% ../nbs/02_model.ipynb 3
import os

import torch
import torch.nn as nn
from torchvision import transforms, models



# %% ../nbs/02_model.ipynb 4
def build_model(backbone="resnet18", num_classes=3, pretrained=True):

    if backbone == "resnet18":
        model = models.resnet18(pretrained=pretrained)
        model.fc = nn.Linear(model.fc.in_features, num_classes)
    elif backbone == "efficientnet":
        model = models.efficientnet_b0(pretrained=pretrained)
        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)
    else:
        raise ValueError("Unsupported backbone")
    return model

# %% ../nbs/02_model.ipynb 5
def load_ckpt(model, ckpt_path):
    if os.path.exists(ckpt_path):
        state_dict = torch.load(ckpt_path, map_location='cpu')
        model.load_state_dict(state_dict)
        print(f"Loaded pretrained weights from {ckpt_path}")
    else:
        print(f"No checkpoint found at {ckpt_path}. Using random initialized weights.")
    return model

# %% ../nbs/02_model.ipynb 6
def init_model(cfg): 
    
    model = build_model(backbone=cfg.model.backbone, num_classes=len(cfg.data.label_names), pretrained=cfg.model.pretrained)
    model = load_ckpt(model, cfg.model.ckpt)
    for param in model.parameters():
        param.requires_grad = False

    if cfg.task == "eval":
        return model

    if cfg.task == 'probing':
        key = "fc" if cfg.model.backbone == "resnet18" else "classifier.1"
        for param in getattr(model, key).parameters():
            param.requires_grad = True

    elif cfg.task == 'fine-tuning':
        for param in model.parameters():
            param.requires_grad = True

    else:
        raise ValueError("Unsupported type. Choose either 'eval', 'probing', or 'fine-tuning'.")

    return model

# %% ../nbs/02_model.ipynb 9
import torch
import torch.nn as nn
from torchvision import models

class ResNet18WithAttention(nn.Module):
    def __init__(self, num_classes=1000, num_heads=8):
        super(ResNet18WithAttention, self).__init__()
        resnet = models.resnet18(weights=None)
        self.backbone = nn.Sequential(*list(resnet.children())[:-2]) # Stops at (B, 512, 8, 8)
        
        self.embed_dim = 512
        
        self.mha = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=num_heads, batch_first=True)
        
        self.ln = nn.LayerNorm(self.embed_dim)
        self.fc = nn.Linear(self.embed_dim, num_classes)

    def forward(self, x):
        # x shape: (Batch, 3, 256, 256)
        features = self.backbone(x) # Shape: (Batch, 512, 8, 8)
        
        # Flatten spatial dimensions: (B, 512, 8, 8) -> (B, 512, 64) -> (B, 64, 512)
        # We treat the 64 pixels as the "sequence"
        b, c, h, w = features.shape
        features = features.view(b, c, h * w).permute(0, 2, 1) 
        
        attn_output, _ = self.mha(features, features, features)
        
        x = self.ln(attn_output + features)
        
        x = x.mean(dim=1)
        
        logits = self.fc(x)
        return logits



# %% ../nbs/02_model.ipynb 13
import torch
import torch.nn as nn
from torchvision import models

class EfficientNetB0WithAttention(nn.Module):
    def __init__(self, num_classes=1000, num_heads=8):
        super(EfficientNetB0WithAttention, self).__init__()
        
        # 1. Load EfficientNet_B0
        # We take only the 'features' part to exclude the default pooling/classifier
        effnet = models.efficientnet_b0(weights=None)
        self.backbone = effnet.features 
        
        # EfficientNet-B0 final stage outputs 1280 channels
        self.embed_dim = 1280 
        
        # 2. Multi-Head Attention Layer
        # Ensure embed_dim is divisible by num_heads (1280 / 8 = 160)
        self.mha = nn.MultiheadAttention(
            embed_dim=self.embed_dim, 
            num_heads=num_heads, 
            batch_first=True
        )
        
        # 3. Normalization and Classifier
        self.ln = nn.LayerNorm(self.embed_dim)
        self.fc = nn.Linear(self.embed_dim, num_classes)

    def forward(self, x):
        # x shape: (Batch, 3, 256, 256)
        # EfficientNet features output shape: (Batch, 1280, 8, 8)
        features = self.backbone(x)
        
        b, c, h, w = features.shape
        # Flatten spatial: (B, 1280, 64) -> Permute: (B, 64, 1280)
        features = features.view(b, c, h * w).permute(0, 2, 1)
        
        # Apply Self-Attention
        attn_output, _ = self.mha(features, features, features)
        
        # Residual connection + Layer Norm
        x = self.ln(attn_output + features)
        
        # Global Average Pooling (Across the 64 spatial tokens)
        x = x.mean(dim=1) 
        
        # Classification
        logits = self.fc(x)
        return logits


