"""Main logic for trainer (Fit, predict)"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_trainer.ipynb.

# %% auto 0
__all__ = ['Trainer']

# %% ../nbs/04_trainer.ipynb 3
from fastcore.all import *
from fastcore.utils import *

# %% ../nbs/04_trainer.ipynb 4
import pandas as pd
import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score
import wandb


# %% ../nbs/04_trainer.ipynb 5
class Trainer:
    def __init__(self, cfg, model, loaders, criterion, optimizer, scheduler, early_stopper, device, writer):
        self.cfg = cfg
        self.model = model
        self.train_loader = loaders['train']
        self.val_loader = loaders['val']
        self.test_loader = loaders['test']
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.early_stopper = early_stopper
        self.device = device
        self.writer = writer

# %% ../nbs/04_trainer.ipynb 6
@patch
def train(self:Trainer):
    self.model.train()
    running_loss = 0.0
    for inputs, labels in self.train_loader:
        inputs, labels = inputs.to(self.device), labels.to(self.device)
        
        if self.cfg.data.V > 1:
            B, V, C, H, W = inputs.shape
            labels = labels.repeat_interleave(V, dim=0)

        self.optimizer.zero_grad()
        outputs = self.model(inputs)
        loss = self.criterion(outputs, labels)
        loss.backward()
        self.optimizer.step()

        running_loss += loss.item() * inputs.size(0)
    epoch_loss = running_loss / len(self.train_loader.dataset)
    return epoch_loss


# %% ../nbs/04_trainer.ipynb 7
@patch
def eval_(self: Trainer):
    self.model.eval()
    running_loss = 0.0
    with torch.no_grad():
        for inputs, labels in self.val_loader:
            inputs, labels = inputs.to(self.device), labels.to(self.device)

            outputs = self.model(inputs)
            loss = self.criterion(outputs, labels)

            running_loss += loss.item() * inputs.size(0)

    epoch_loss = running_loss / len(self.val_loader.dataset)
    return epoch_loss


# %% ../nbs/04_trainer.ipynb 8
@patch
def predict(self: Trainer):
    self.model.to(self.device)
    self.model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for imgs, labels in self.test_loader:
            imgs = imgs.to(self.device)
            outputs = self.model(imgs)
            probs = torch.sigmoid(outputs).cpu().numpy()
            preds = (probs > 0.5).astype(int)
            y_true.extend(labels.numpy())
            y_pred.extend(preds)

    y_true = torch.tensor(y_true).numpy()
    y_pred = torch.tensor(y_pred).numpy()

    res = {}
    for i, disease in enumerate(self.cfg.data.label_names):  #compute metrics for every disease
        y_t = y_true[:, i]
        y_p = y_pred[:, i]

        acc = accuracy_score(y_t, y_p)
        precision = precision_score(y_t, y_p, average="macro",zero_division=0)
        recall = recall_score(y_t, y_p, average="macro",zero_division=0)
        f1 = f1_score(y_t, y_p, average="macro",zero_division=0)
        kappa = cohen_kappa_score(y_t, y_p)

        print(f"{disease} Results [{self.cfg.model.backbone}]")
        print(f"Accuracy : {acc:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall   : {recall:.4f}")
        print(f"F1-score : {f1:.4f}")
        print(f"Kappa    : {kappa:.4f}")

        res[disease] = {
            "accuracy": acc,
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "cohen_kappa": kappa
        }    

    avg_acc = accuracy_score(y_true, y_pred)
    avg_precision = precision_score(y_true, y_pred, average="macro",zero_division=0)
    avg_recall = recall_score(y_true, y_pred, average="macro",zero_division=0)
    avg_f1 = f1_score(y_true, y_pred, average="macro",zero_division=0)
    avg_kappa = -1.#cohen_kappa_score(y_true, y_pred) # TODO: Check if kappa can be averaged across multi-label
    res['avg'] = {"f1_score": avg_f1, "accuracy": avg_acc, "precision": avg_precision, "recall": avg_recall, "cohen_kappa": avg_kappa}

    metrics_df = pd.DataFrame(res).T
    df_result = metrics_df.reset_index()
    self.writer.write({'Test Metrics': wandb.Table(dataframe= df_result)})
    return df_result

# %% ../nbs/04_trainer.ipynb 9
@patch
def fit(self: Trainer):
    verb = "probing" if self.cfg.task == "probing" else "fine-tuning"
    print(f"Starting {verb} the model")
    print("***" * 10)
    self.model.to(self.device)
    train_losses = []
    val_losses = []
    for epoch in range(self.cfg.epochs):
        self.model.train()
        train_loss = self.train()
        val_loss = self.eval_()
        self.scheduler.step(val_loss)
        
        
        to_log = {
            'train_loss': train_loss,
            'val_loss': val_loss,
        }

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        if self.early_stopper.early_stop(val_loss):             
            break
        
        self.writer.write(to_log)

    return self.model, self.optimizer, train_losses, val_losses
